{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %reset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Includes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import glob\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import colors\n",
    "import mediapipe as mp\n",
    "# import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import svm\n",
    "from skimage.morphology import convex_hull_image\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "import cv2\n",
    "# from PIL import Image\n",
    "from os import listdir"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compress images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compress_image(path : str):\n",
    "# \tim = Image.open(path)\n",
    "# \tim.thumbnail((500, 500), Image.LANCZOS)\n",
    "# \tim.save(path.replace('dataset', 'reduced_dataset'))\n",
    "\n",
    "# def compress_dataset(path : str):\n",
    "# \tfor num in range(0,6):\n",
    "# \t\tmen_dir = path + f'men/{num}/'\n",
    "# \t\twomen_dir = path + f'Women/{num}/'\n",
    "# \t\tfor file in listdir(men_dir):\n",
    "# \t\t\tcompress_image(men_dir + file)\n",
    "# \t\tfor file in listdir(women_dir):\n",
    "# \t\t\tcompress_image(women_dir + file)\n",
    "\n",
    "# compress_dataset('../../dataset/')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 72\u001b[0m\n\u001b[0;32m     69\u001b[0m \t\tarray \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate((array, new_men, new_women), axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m     70\u001b[0m \t\u001b[39mreturn\u001b[39;00m array\n\u001b[1;32m---> 72\u001b[0m dataset \u001b[39m=\u001b[39m read_images(\u001b[39m'\u001b[39;49m\u001b[39m../clean_reduced_dataset/\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[10], line 67\u001b[0m, in \u001b[0;36mread_images\u001b[1;34m(directory)\u001b[0m\n\u001b[0;32m     65\u001b[0m men_dir \u001b[39m=\u001b[39m directory \u001b[39m+\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmen/\u001b[39m\u001b[39m{\u001b[39;00mnum\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     66\u001b[0m women_dir \u001b[39m=\u001b[39m directory \u001b[39m+\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mWomen/\u001b[39m\u001b[39m{\u001b[39;00mnum\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m---> 67\u001b[0m new_men \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([np\u001b[39m.\u001b[39mconcatenate((np\u001b[39m.\u001b[39masarray([num]), read_image(men_dir \u001b[39m+\u001b[39m file)), axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m) \u001b[39mfor\u001b[39;00m file \u001b[39min\u001b[39;00m listdir(men_dir)])\n\u001b[0;32m     68\u001b[0m new_women \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([np\u001b[39m.\u001b[39mconcatenate((np\u001b[39m.\u001b[39masarray([num]), read_image(women_dir \u001b[39m+\u001b[39m file)), axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m) \u001b[39mfor\u001b[39;00m file \u001b[39min\u001b[39;00m listdir(women_dir)])\n\u001b[0;32m     69\u001b[0m array \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate((array, new_men, new_women), axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "Cell \u001b[1;32mIn[10], line 67\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     65\u001b[0m men_dir \u001b[39m=\u001b[39m directory \u001b[39m+\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmen/\u001b[39m\u001b[39m{\u001b[39;00mnum\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     66\u001b[0m women_dir \u001b[39m=\u001b[39m directory \u001b[39m+\u001b[39m \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mWomen/\u001b[39m\u001b[39m{\u001b[39;00mnum\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m---> 67\u001b[0m new_men \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([np\u001b[39m.\u001b[39mconcatenate((np\u001b[39m.\u001b[39masarray([num]), read_image(men_dir \u001b[39m+\u001b[39;49m file)), axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m) \u001b[39mfor\u001b[39;00m file \u001b[39min\u001b[39;00m listdir(men_dir)])\n\u001b[0;32m     68\u001b[0m new_women \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([np\u001b[39m.\u001b[39mconcatenate((np\u001b[39m.\u001b[39masarray([num]), read_image(women_dir \u001b[39m+\u001b[39m file)), axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m) \u001b[39mfor\u001b[39;00m file \u001b[39min\u001b[39;00m listdir(women_dir)])\n\u001b[0;32m     69\u001b[0m array \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mconcatenate((array, new_men, new_women), axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m, in \u001b[0;36mread_image\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mread_image\u001b[39m(path: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m np\u001b[39m.\u001b[39marray:\n\u001b[1;32m----> 2\u001b[0m \timg \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39;49mimread(path)\n\u001b[0;32m      3\u001b[0m \t\u001b[39m# img_gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \t\u001b[39m# dx = cv2.Sobel(img_gray,cv2.CV_64F,1,0,ksize=9)\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \t\u001b[39m# dy = cv2.Sobel(img_gray,cv2.CV_64F,0,1,ksize=9)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     11\u001b[0m \t\u001b[39m# # hist_bin *= 1/np.max(hist_bin)\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \t\u001b[39m# hist_bin = np.sum(hist_bin.reshape((-1,2)), axis=1)\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \timg \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mcvtColor(img,cv2\u001b[39m.\u001b[39mCOLOR_BGR2YCrCb)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def read_image(path: str) -> np.array:\n",
    "\timg = cv2.imread(path)\n",
    "\t# img_gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "\t# dx = cv2.Sobel(img_gray,cv2.CV_64F,1,0,ksize=9)\n",
    "\t# dy = cv2.Sobel(img_gray,cv2.CV_64F,0,1,ksize=9)\n",
    "\t# mag, angle = cv2.cartToPolar(dx,dy,angleInDegrees=True)\n",
    "\t# # mag *= 255.0/np.max(mag)\n",
    "\t# hist_bin = np.zeros((12,))\n",
    "\t# for i in range(0,12):\n",
    "\t# \thist_bin[i] = np.sum(mag[(angle>=i*30) & (angle<(i+1)*30)])\n",
    "\t# # hist_bin *= 1/np.max(hist_bin)\n",
    "\t# hist_bin = np.sum(hist_bin.reshape((-1,2)), axis=1)\n",
    "\n",
    "\timg = cv2.cvtColor(img,cv2.COLOR_BGR2YCrCb)\n",
    "\tavg_luma = np.mean(img[:,:,0])\n",
    "\tif avg_luma>196:\n",
    "\t\timg = cv2.inRange(img,(0,133,77),(255,163,140))\n",
    "\telse:\n",
    "\t\timg = cv2.inRange(img,(0,137,77),(255,163,140))\n",
    "\tblur = cv2.medianBlur(img, 5)\n",
    "\tkernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (8, 8))\n",
    "\timg_filtered = cv2.dilate(blur, kernel)\n",
    "\tctrs, _ = cv2.findContours(img,cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_NONE)\n",
    "\tmax_ctr = max(ctrs,key=cv2.contourArea)\n",
    "\thullpts = np.zeros((40,2))\n",
    "\thull = cv2.convexHull(max_ctr)\n",
    "\tcontour = np.zeros(img.shape)\n",
    "\t\n",
    "\tcv2.drawContours(contour,hull,-1,(255,255,255),1)\n",
    "\tcontour = convex_hull_image(contour)\n",
    "\tcontour = np.asarray(hull,dtype=int)\n",
    "\tcontour = np.sum(contour)\n",
    "\thull = hull.reshape(-1,2)\n",
    "\thullpts[:hull.shape[0],:] = hull\n",
    "\t# img = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "\t# dx = cv2.Sobel(img,cv2.CV_64F,1,0,ksize=1)\n",
    "\t# dy = cv2.Sobel(img,cv2.CV_64F,0,1,ksize=1)\n",
    "\t# img_filtered = np.hypot(dx,dy)\n",
    "\t# img_filtered *= 255.0/np.max(img_filtered)\n",
    "\t# img_filtered = cv2.threshold(img_filtered,thresh=50,maxval=255,type=cv2.THRESH_BINARY)[1]\n",
    "\t# histogramV = np.array([np.mean(np.sum(img_filtered,axis=0))])\n",
    "\t# img = cv2.cvtColor(img,cv2.COLOR_BGR2YCrCb)\n",
    "\t# img = cv2.cvtColor(img,cv2.COLOR_BGR2YCrCb)\n",
    "\t# img_gray = cv2.inRange(cv2.cvtColor(img,cv2.COLOR_BGR2GRAY),0,128)\n",
    "\t# brightness_coeff = (np.mean(img_gray)/255 - 0.015)*10.25\n",
    "\t# img_filtered = cv2.inRange(img,(0,int(133+brightness_coeff),77),(255,163,140))\n",
    "\t# print(img_filtered_cluttered[5][0])\n",
    "\t# img = cv2.cvtColor(img,cv2.COLOR_BGR2HSV)\n",
    "\t# img_masked_shadow = np.bitwise_not(cv2.inRange(img,(0,0,80),(255,55,179)))\n",
    "\t# img_masked_hand = cv2.inRange(img,(5,35,30),(25,255,255))\n",
    "\t# img_filtered = np.bitwise_and(img_masked_shadow,img_masked_hand)\n",
    "\t# area = np.array([np.sum(img_filtered.reshape((-1))) / 255])\n",
    "\t# bucket_size = 10\n",
    "\t# img_filtered_cluttered = img_filtered[1:,:].reshape((int(img_filtered[1:,:].shape[0]/bucket_size),-1,bucket_size))\n",
    "\t# histogramH = np.array([np.mean(np.diff(np.sum(img_filtered_cluttered,axis=(1,2))))])\n",
    "\t# # print(histogramH.dtype)\n",
    "\t# bucket_size = 20\n",
    "\t# img_filtered_cluttered = img_filtered.reshape((-1,int(img_filtered.shape[1]/bucket_size),bucket_size))\n",
    "\treturn np.concatenate((hullpts[:,0],hullpts[:,1],[contour],img_filtered.reshape(-1)))\n",
    "\t# return np.concatenate((hullpts[:,0],hullpts[:,1],[contour],hist_bin,img_filtered.reshape(-1)))\n",
    "\n",
    "def read_images(directory : str) -> np.array:\n",
    "\tarray = np.array([], dtype=np.int8).reshape(0, 140582)\n",
    "\tfor num in range(0,6):\n",
    "\t\tmen_dir = directory + f'men/{num}/'\n",
    "\t\twomen_dir = directory + f'Women/{num}/'\n",
    "\t\tnew_men = np.array([np.concatenate((np.asarray([num]), read_image(men_dir + file)), axis=0) for file in listdir(men_dir)])\n",
    "\t\tnew_women = np.array([np.concatenate((np.asarray([num]), read_image(women_dir + file)), axis=0) for file in listdir(women_dir)])\n",
    "\t\tarray = np.concatenate((array, new_men, new_women), axis=0)\n",
    "\treturn array\n",
    "\n",
    "dataset = read_images('../reduced_dataset/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, validation = train_test_split(dataset, test_size=0.1)\n",
    "X = train[:, 1:]\n",
    "y = train[:, 0]\n",
    "X_val = validation[:, 1:]\n",
    "y_val = validation[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display_html\n",
    "from itertools import chain,cycle\n",
    "def display_side_by_side(*args,titles=cycle([''])):\n",
    "    html_str=''\n",
    "    for df,title in zip(args, chain(titles,cycle(['</br>'])) ):\n",
    "        # html_str+='<th style=\"text-align:center\"><td style=\"vertical-align:top\">'\n",
    "        # html_str+=f'<h2 style=\"text-align: center;\">{title}</h2>'\n",
    "        html_str+=df.to_html().replace('table','table style=\"display:inline\"')\n",
    "        # html_str+='</td></th>'\n",
    "    display_html(html_str,raw=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "John's Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count_arr = np.zeros((6,6))\n",
    "def model_prediction(model, x_training, y_training, x_validation, y_validation):\n",
    "\tmodel.fit(x_training, y_training)\n",
    "\tprediction_training = np.array(np.floor(model.predict(x_training)), dtype=np.int8)\n",
    "\tprediction_validation = np.array(np.floor(model.predict(x_validation)), dtype=np.int8)\n",
    "\tacc_training = accuracy_score(y_training, prediction_training)\n",
    "\tacc_validation = accuracy_score(y_validation, prediction_validation)\n",
    "\n",
    "\tconfusion_matrix_training = confusion_matrix(y_training, prediction_training)\n",
    "\tconfusion_matrix_validation = confusion_matrix(y_validation, prediction_validation)\n",
    "\t# print(\"Confusion Matrix Training: \\n\", confusion_matrix_training)\n",
    "\tdisplay_side_by_side(pd.DataFrame(confusion_matrix_training), pd.DataFrame(confusion_matrix_validation))\n",
    "\t# print(\"Confusion Matrix Validation: \\n\", confusion_matrix_validation)\n",
    "\tprint(\"Accuracuy Score Training: \", acc_training)\n",
    "\tprint(\"Accuracy Score Validation: \",acc_validation)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=30).fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 140581 features, but SVC is expecting 30 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model_prediction(svm\u001b[39m.\u001b[39;49mSVC(C\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m), pca\u001b[39m.\u001b[39;49mtransform(X), y, X, y)\n\u001b[0;32m      2\u001b[0m \u001b[39m# for i in range(1, 6):\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39m#     train, validation = train_test_split(dataset, test_size=0.2)\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39m#     X = train[:, 1:]\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[39m# print('KNN')\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[39m# model_prediction(KNeighborsClassifier(n_neighbors=6), X, y, X_val, y_val)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[6], line 6\u001b[0m, in \u001b[0;36mmodel_prediction\u001b[1;34m(model, x_training, y_training, x_validation, y_validation)\u001b[0m\n\u001b[0;32m      4\u001b[0m pickle\u001b[39m.\u001b[39mdump(model, \u001b[39mopen\u001b[39m( \u001b[39m\"\u001b[39m\u001b[39mclassifier.pkl\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m\"\u001b[39m ))\n\u001b[0;32m      5\u001b[0m prediction_training \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(np\u001b[39m.\u001b[39mfloor(model\u001b[39m.\u001b[39mpredict(x_training)), dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mint8)\n\u001b[1;32m----> 6\u001b[0m prediction_validation \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(np\u001b[39m.\u001b[39mfloor(model\u001b[39m.\u001b[39;49mpredict(x_validation)), dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mint8)\n\u001b[0;32m      7\u001b[0m acc_training \u001b[39m=\u001b[39m accuracy_score(y_training, prediction_training)\n\u001b[0;32m      8\u001b[0m acc_validation \u001b[39m=\u001b[39m accuracy_score(y_validation, prediction_validation)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\svm\\_base.py:820\u001b[0m, in \u001b[0;36mBaseSVC.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    818\u001b[0m     y \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecision_function(X), axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m    819\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 820\u001b[0m     y \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mpredict(X)\n\u001b[0;32m    821\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses_\u001b[39m.\u001b[39mtake(np\u001b[39m.\u001b[39masarray(y, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mintp))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\svm\\_base.py:433\u001b[0m, in \u001b[0;36mBaseLibSVM.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    417\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, X):\n\u001b[0;32m    418\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Perform regression on samples in X.\u001b[39;00m\n\u001b[0;32m    419\u001b[0m \n\u001b[0;32m    420\u001b[0m \u001b[39m    For an one-class model, +1 (inlier) or -1 (outlier) is returned.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    431\u001b[0m \u001b[39m        The predicted values.\u001b[39;00m\n\u001b[0;32m    432\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 433\u001b[0m     X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_for_predict(X)\n\u001b[0;32m    434\u001b[0m     predict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sparse_predict \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sparse \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dense_predict\n\u001b[0;32m    435\u001b[0m     \u001b[39mreturn\u001b[39;00m predict(X)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\svm\\_base.py:613\u001b[0m, in \u001b[0;36mBaseLibSVM._validate_for_predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    610\u001b[0m check_is_fitted(\u001b[39mself\u001b[39m)\n\u001b[0;32m    612\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m callable(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkernel):\n\u001b[1;32m--> 613\u001b[0m     X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[0;32m    614\u001b[0m         X,\n\u001b[0;32m    615\u001b[0m         accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    616\u001b[0m         dtype\u001b[39m=\u001b[39;49mnp\u001b[39m.\u001b[39;49mfloat64,\n\u001b[0;32m    617\u001b[0m         order\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mC\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    618\u001b[0m         accept_large_sparse\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    619\u001b[0m         reset\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[0;32m    620\u001b[0m     )\n\u001b[0;32m    622\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sparse \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m sp\u001b[39m.\u001b[39misspmatrix(X):\n\u001b[0;32m    623\u001b[0m     X \u001b[39m=\u001b[39m sp\u001b[39m.\u001b[39mcsr_matrix(X)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\base.py:569\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    566\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[0;32m    568\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m--> 569\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_n_features(X, reset\u001b[39m=\u001b[39;49mreset)\n\u001b[0;32m    571\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\sklearn\\base.py:370\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    367\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m    369\u001b[0m \u001b[39mif\u001b[39;00m n_features \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_features_in_:\n\u001b[1;32m--> 370\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    371\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX has \u001b[39m\u001b[39m{\u001b[39;00mn_features\u001b[39m}\u001b[39;00m\u001b[39m features, but \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    372\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mis expecting \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_features_in_\u001b[39m}\u001b[39;00m\u001b[39m features as input.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    373\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: X has 140581 features, but SVC is expecting 30 features as input."
     ]
    }
   ],
   "source": [
    "for i in range(1, 6):\n",
    "    train, validation = train_test_split(dataset, test_size=0.2)\n",
    "    X = train[:, 1:]\n",
    "    y = train[:, 0]\n",
    "    X_val = validation[:, 1:]\n",
    "    y_val = validation[:, 0]\n",
    "    print(f'Run no.{i}:')\n",
    "    pca = PCA(n_components=30).fit(X)\n",
    "    model_prediction(svm.SVC(C=10), pca.transform(X), y, pca.transform(X_val), y_val)\n",
    "    # m = preprocessing.StandardScaler().fit(X)\n",
    "    # X, X_val = m.transform(X),m.transform(X_val)\n",
    "# print('Random forest classifier')\n",
    "# for i in range(1, 6):\n",
    "# \tprint(f'Run no.{i}:')\n",
    "# \tmodel_prediction(RandomForestClassifier(n_estimators = 500, min_samples_split=10, max_features=None), pca.transform(X), y, pca.transform(X_val), y_val)\n",
    "# model_prediction(RandomForestClassifier(n_estimators = 500, min_samples_split=10, max_features=None), pca.transform(X), y, pca.transform(X_val), y_val)\n",
    "# print('SVC')\n",
    "# model_prediction(svm.SVC(), pca.transform(X), y, pca.transform(X_val), y_val)\n",
    "# print('NuSVC')\n",
    "# model_prediction(svm.NuSVC(nu=0.1), pca.transform(X), y, pca.transform(X_val), y_val)\n",
    "# print('SVC with C=10')\n",
    "# model_prediction(svm.SVC(C=10), pca.transform(X), y, pca.transform(X_val), y_val)\n",
    "# print('Random forest regressor')\n",
    "# model_prediction(RandomForestRegressor(n_estimators = 500, min_samples_split=10, max_features=None), pca.transform(X), y, pca.transform(X_val), y_val)\n",
    "\n",
    "# print('KNN')\n",
    "# model_prediction(KNeighborsClassifier(n_neighbors=6), pca.transform(X), y, pca.transform(X_val), y_val)\n",
    "# dataset = np.array([])\n",
    "# for filename in sorted(glob.glob('../clean_reduced_dataset/men/*/*.JPG')):\n",
    "# \timg = cv2.imread(filename)\n",
    "# \timg = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "# \tdx = cv2.Sobel(img,cv2.CV_64F,1,0,ksize=7)\n",
    "# \tdy = cv2.Sobel(img,cv2.CV_64F,0,1,ksize=7)\n",
    "# \timg_masked = np.linalg.norm(np.array([dx,dy]),axis=0)\n",
    "# \timg_masked *= 255.0/np.max(img_masked)\n",
    "# \timg_masked = np.asarray(cv2.threshold(img_masked,thresh=100,maxval=255,type=cv2.THRESH_BINARY)[1],dtype=np.uint8)\n",
    "\n",
    "# \tctrs, _ = cv2.findContours(img_masked,cv2.RETR_TREE,cv2.CHAIN_APPROX_NONE)\n",
    "# \tmax_ctr = max(ctrs,key=cv2.contourArea)\n",
    "# \tcontour = np.zeros(img_masked.shape)\n",
    "# \tcv2.drawContours(contour,max_ctr,-1,(255,0,0),1)\n",
    "# \t# print(cv2.contourArea(max_ctr))\n",
    "# \t# histogramV = np.mean(np.sum(img_masked,axis=0))\n",
    "# \tlabel = int(filename[29])\n",
    "# \tarray = np.array([label,cv2.contourArea(max_ctr)])\n",
    "# \tdataset = np.concatenate((dataset,array),axis=0)\n",
    "# dataset = dataset.reshape(-1,2)\n",
    "# cmap = colors.ListedColormap(['red','green','blue','yellow','cyan','pink'])\n",
    "# plt.scatter(dataset[:,0],dataset[:,1])\n",
    "# train, validation = train_test_split(dataset, test_size=0.1)\n",
    "# X = train[:, 1:]\n",
    "# y = train[:, 0]\n",
    "# X_val = validation[:, 1:]\n",
    "# y_val = validation[:, 0]\n",
    "# print('KNN')\n",
    "# model_prediction(KNeighborsClassifier(n_neighbors=6), X, y, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10, 50, 2):\n",
    "\tprint('*'* 20)\n",
    "\tprint(f'PCA {i} components')\n",
    "\tpca = PCA(n_components=i).fit(X)\n",
    "\tprint('SVM')\n",
    "\tmodel_prediction(svm.SVC(), pca.transform(X), y, pca.transform(X_val), y_val)\n",
    "\t# print('KNN, N=5')\n",
    "\t# model_prediction(KNeighborsClassifier(n_neighbors=5), pca.transform(X), y, pca.transform(X_val), y_val)\n",
    "\tprint('*'* 20)\n",
    "\tprint('\\n\\n')\n",
    "# 78.88%  85%  84.44%  77.77%  79.44%  with hull\n",
    "# 74.44%  75.55%  77.22%  82.22%  81.66%  without hull"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
