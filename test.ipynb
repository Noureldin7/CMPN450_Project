{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import math\n",
    "import glob\n",
    "from sklearn.cluster import MeanShift, estimate_bandwidth, KMeans\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.cvtColor(cv2.imread(\"../reduced_dataset/men/4/4_men (141).JPG\"),cv2.COLOR_BGR2YCrCb)\n",
    "# plt.imshow(img)\n",
    "# img = cv2.resize(src=img,dsize=(int(img.shape[1]/8),int(img.shape[0]/8)))\n",
    "# img = cv2.bilateralFilter(img,d=5,sigmaColor=10,sigmaSpace=10)\n",
    "img_vector = img.reshape([-1,3])\n",
    "# img_vector = img_vector[:,1:3]\n",
    "img_vector[:,0] = 100\n",
    "# img_vector[:,2] = img_vector[:,2] * 12\n",
    "# img_vector[:,1] = img_vector[:,1] * 6\n",
    "# img_vector_YExcluded = img_vector[:,1:]\n",
    "bw = estimate_bandwidth(X=img_vector,quantile=0.1,n_samples=500)\n",
    "# bw\n",
    "print(bw)\n",
    "ms = MeanShift(bandwidth=bw,bin_seeding=True)\n",
    "ms.fit(img_vector)\n",
    "print(ms.cluster_centers_.shape)\n",
    "# img\n",
    "\n",
    "# img_vector[:,1:3] = img_vector[:,1:3] / 5\n",
    "result_image = ms.cluster_centers_.astype(dtype=np.uint8)[ms.labels_].reshape((img.shape[0],img.shape[1],3))\n",
    "# result_image = ms.cluster_centers_.astype(dtype=np.uint8)[ms.labels_].reshape((img.shape[0],img.shape[1],2))\n",
    "# result_image = cv2.cvtColor(result_image,cv2.COLOR_YCrCb2BGR)\n",
    "# result_image = cv2.inRange(result_image,(0,137,77),(255,163,140))\n",
    "# result_image[:,2] = result_image[:,2] / 12\n",
    "# result_image[:,1] = result_image[:,1] / 6\n",
    "# result_image = np.concatenate(np.ones((img.shape[0], img.shape[1],1),dtype=np.uint8)*128,result_image)\n",
    "cv2.imwrite(\"compressed.JPG\",result_image)\n",
    "# plt.imshow(result_image)\n",
    "# print(ms.cluster_centers_[ms.labels_])\n",
    "# fig = plt.figure(figsize=(12,10))\n",
    "# ax = fig.add_subplot(111,projection='3d')\n",
    "# ax.scatter(img[0,:,0],img[0,:,1],img[0,:,2])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.cvtColor(cv2.imread(\"../reduced_dataset/men/4/4_men (141).JPG\"),cv2.COLOR_BGR2YCrCb)\n",
    "bg_mask = np.bitwise_not(cv2.inRange(img,(0,137,77),(255,163,120))).reshape((-1))\n",
    "fg_mask = cv2.inRange(img,(0,137,77),(255,163,120)).reshape((-1))\n",
    "# print(bg_mask.shape)\n",
    "# print(img.reshape((-1,3)).shape)\n",
    "# print(img.reshape((-1,3))[bg_mask == 255].shape)\n",
    "bg_mean = np.mean(img.reshape((-1,3))[bg_mask == 255], axis = 0)\n",
    "fg_mean = np.mean(img.reshape((-1,3))[fg_mask != 0], axis = 0)\n",
    "print(bg_mean)\n",
    "print(fg_mean)\n",
    "\n",
    "# plt.imshow(img)\n",
    "# img = cv2.resize(src=img,dsize=(int(img.shape[1]/8),int(img.shape[0]/8)))\n",
    "# img = cv2.bilateralFilter(img,d=5,sigmaColor=10,sigmaSpace=10)\n",
    "img_vector = img.reshape([-1,3])\n",
    "# img_vector[:,0] = 100\n",
    "divisor = 20\n",
    "img_vector[:,1] = (np.tanh((img_vector[:,1] - 137)/divisor) - np.tanh((img_vector[:,1] - 163)/divisor)) * img_vector[:,1]/2\n",
    "img_vector[:,2] = (np.tanh((img_vector[:,2] - 77)/divisor) - np.tanh((img_vector[:,2] - 120)/divisor)) * img_vector[:,2]/2\n",
    "# (0,137,77),(255,163,120)\n",
    "# cv2.imwrite(\"compressed.JPG\",img_vector.reshape((img.shape[0],img.shape[1],3)))\n",
    "\n",
    "\n",
    "# km = KMeans(n_clusters=3)\n",
    "km = KMeans(n_clusters=2, init=np.array([[bg_mean[0],0,0], fg_mean]), max_iter=3)\n",
    "# km = KMeans(n_clusters=3, init=np.array([bg_mean,[30, 149, 96],[200, 149, 96]]), max_iter=3)\n",
    "km.fit(img_vector)\n",
    "\n",
    "result_image = km.cluster_centers_.astype(dtype=np.uint8)[km.labels_].reshape((img.shape[0],img.shape[1],3))\n",
    "# result_image = cv2.cvtColor(result_image,cv2.COLOR_YCrCb2BGR)\n",
    "cv2.imwrite(\"compressed.JPG\",result_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=0\n",
    "for filename in sorted(glob.glob('data_set/men/0/*.JPG')):\n",
    "    img = cv2.imread(filename) \n",
    "    img = cv2.cvtColor(img,cv2.COLOR_BGR2YCrCb)\n",
    "    img_masked = cv2.inRange(img,(0,137,77),(255,163,120))\n",
    "    cv2.imwrite(\"masks/\"+str(x)+\".jpg\",img_masked)\n",
    "    x+=1\n",
    "# plt.imshow(img_masked)\n",
    "# np.array((0,137,77),dtype=np.uint8)\n",
    "# img_masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = cv2.imread(\"reduced_dataset/men/3/3_men (64).JPG\")\n",
    "# img = cv2.cvtColor(img,cv2.COLOR_BGR2YCrCb)\n",
    "# img_masked = cv2.inRange(img,(0,136,77),(255,163,140))\n",
    "img = cv2.cvtColor(img,cv2.COLOR_BGR2HSV)\n",
    "img_masked_shadow = np.bitwise_not(cv2.inRange(img,(0,0,80),(255,55,178)))\n",
    "img_masked_hand = cv2.inRange(img,(5,35,30),(25,255,255))\n",
    "img_masked = np.bitwise_and(img_masked_shadow,img_masked_hand)\n",
    "cv2.imwrite(\"masked.JPG\",img_masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "img = cv.imread('D:/pattern/research/dataset_sample/men/3/3_men (2).JPG', cv.IMREAD_GRAYSCALE)\n",
    "assert img is not None, \"file could not be read, check with os.path.exists()\"\n",
    "# global thresholding\n",
    "ret1,th1 = cv.threshold(img,127,255,cv.THRESH_BINARY)\n",
    "# Otsu's thresholding\n",
    "ret2,th2 = cv.threshold(img,0,255,cv.THRESH_BINARY+cv.THRESH_OTSU)\n",
    "# Otsu's thresholding after Gaussian filtering\n",
    "blur = cv.GaussianBlur(img,(5,5),0)\n",
    "ret3,th3 = cv.threshold(blur,0,255,cv.THRESH_BINARY+cv.THRESH_OTSU)\n",
    "# plot all the images and their histograms\n",
    "images = [img, 0, th1,\n",
    "          img, 0, th2,\n",
    "          blur, 0, th3]\n",
    "titles = ['Original Noisy Image','Histogram','Global Thresholding (v=127)',\n",
    "          'Original Noisy Image','Histogram',\"Otsu's Thresholding\",\n",
    "          'Gaussian filtered Image','Histogram',\"Otsu's Thresholding\"]\n",
    "for i in range(3):\n",
    "    plt.subplot(3,3,i*3+1),plt.imshow(images[i*3],'gray')\n",
    "    plt.title(titles[i*3]), plt.xticks([]), plt.yticks([])\n",
    "    plt.subplot(3,3,i*3+2),plt.hist(images[i*3].ravel(),256)\n",
    "    plt.title(titles[i*3+1]), plt.xticks([]), plt.yticks([])\n",
    "    plt.subplot(3,3,i*3+3),plt.imshow(images[i*3+2],'gray')\n",
    "    plt.title(titles[i*3+2]), plt.xticks([]), plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import time\n",
    "\n",
    "# imgWidth = 1280\n",
    "# imgHeight = 720\n",
    "# cap = cv2.VideoCapture(0)\n",
    "# cap.set(3, imgWidth)\n",
    "# cap.set(4, imgHeight)\n",
    "mpHands=mp.solutions.hands\n",
    "hands=mpHands.Hands()\n",
    "mpDraw = mp.solutions.drawing_utils\n",
    "\n",
    "# pTime = 0\n",
    "# cTime = 0\n",
    "filename = 'data_set/men/3/3_men (5).JPG'\n",
    "img = cv2.imread(filename)\n",
    "img= cv2.flip(img,1)\n",
    "img_b = np.empty(img.shape)\n",
    "img_b.fill(0)\n",
    "imgRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "results = hands.process(imgRGB)\n",
    "#print(results.multi_hand_landmarks)\n",
    "if results.multi_hand_landmarks:\n",
    "    for handLms in results.multi_hand_landmarks: \n",
    "    #handLMs are 21 points. so we need conection too-->mpHands.HAND_CONNECTIONS\n",
    "        for id, lm in enumerate(handLms.landmark):\n",
    "            #print(id, lm)\n",
    "            #lm = x,y cordinate of each landmark in float numbers. lm.x, lm.y methods\n",
    "            #So, need to covert in integer\n",
    "            h, w, c =img.shape\n",
    "            cx, cy = int(lm.x * w), int(lm.y * h)\n",
    "            #print(id, cx, cy)\n",
    "            # if id == 4: #(To draw 4th point)\n",
    "            #cv2.circle(img, (cx, cy), 15, (255, 0, 255), cv2.FILLED)\n",
    "        mpDraw.draw_landmarks(img_b, handLms, mpHands.HAND_CONNECTIONS) #drawing points and lines(=handconections)\n",
    "\n",
    "\n",
    "cv2.imwrite('masked.JPG', img_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Parameters\n",
    "# blur = 21\n",
    "# canny_low = 15\n",
    "# canny_high = 150\n",
    "# min_area = 0.0005\n",
    "# max_area = 0.95\n",
    "# dilate_iter = 10\n",
    "# erode_iter = 10\n",
    "# mask_color = (0.0,0.0,0.0)\n",
    "# filename = 'D:/pattern/research/dataset_sample/men/3/3_men (1).JPG'\n",
    "# # Convert image to grayscale        \n",
    "# img = cv2.imread(filename) \n",
    "# image_gray = cv2.cvtColor(img,cv2.COLOR_BGR2YCrCb)\n",
    "# # Apply Canny Edge Dection\n",
    "# edges = cv2.Canny(image_gray, canny_low, canny_high)\n",
    "\n",
    "# edges = cv2.dilate(edges, None)\n",
    "# edges = cv2.erode(edges, None)\n",
    "\n",
    "# contour_info = []\n",
    "\n",
    "# c \n",
    "\n",
    "# for c in cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)[0]:\n",
    "\n",
    "#     contour_info.append((\n",
    "#         c,\n",
    "#         cv2.contourArea(c),\n",
    "#     ))\n",
    "    \n",
    "\n",
    "# # get the contours and their areas\n",
    "\n",
    "\n",
    "# # Set up mask with a matrix of 0's\n",
    "# mask = np.zeros(edges.shape, dtype = np.uint8)\n",
    "\n",
    "\n",
    "# # Go through and find relevant contours and apply to mask\n",
    "# for contour in contour_info:\n",
    "# # Instead of worrying about all the smaller contours, if the area is smaller than the min, the loop will break\n",
    "#     if contour[1] > min_area and contour[1] < max_area:\n",
    "#         # Add contour to mask\n",
    "#         mask = cv2.fillConvexPoly(mask, contour[0], (255))\n",
    "\n",
    "# # use dilate, erode, and blur to smooth out the mask\n",
    "# # use dilate, erode, and blur to smooth out the mask\n",
    "# mask = cv2.dilate(mask, None)\n",
    "# mask = cv2.erode(mask, None)\n",
    "# mask = cv2.GaussianBlur(mask, (blur, blur), 0)\n",
    "\n",
    "# # Ensures data types match up\n",
    "# mask_stack = mask.astype('float32') / 255.0           \n",
    "# img = img.astype('float32') / 255.0\n",
    "\n",
    "# # Creates a 3 channel image\n",
    "\n",
    "# # Multiplies mask_stack with image to get masked image\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(mask_stack.shape)\n",
    "# print(img.shape)\n",
    "\n",
    " \n",
    "# # Blend the image and the mask\n",
    "# masked = (mask_stack * img) + ((1-mask_stack) * mask_color)\n",
    "# masked = (masked * 255).astype('uint8')\n",
    "# cv2.imwrite(\"masked.jpg\", masked)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required modules\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "min_YCrCb = np.array([0,133,77],np.uint8)\n",
    "max_YCrCb = np.array([235,173,140],np.uint8)\n",
    "\n",
    "min_HSV = np.array([0, 58, 30], dtype = \"uint8\")\n",
    "max_HSV = np.array([33, 255, 255], dtype = \"uint8\")\n",
    "\n",
    "# Get pointer to video frames from primary device\n",
    "image = cv2.imread(\"D:/pattern/research/dataset_sample/men/3/3_men (2).JPG\")\n",
    "\n",
    "imageYCrCb = cv2.cvtColor(image,cv2.COLOR_BGR2YCR_CB)\n",
    "skinRegionYCrCb = cv2.inRange(imageYCrCb,min_YCrCb,max_YCrCb)\n",
    "\n",
    "skinYCrCb = cv2.bitwise_and(image, image, mask = skinRegionYCrCb)\n",
    "\n",
    "\n",
    "imageHSV = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "skinRegionHSV = cv2.inRange(imageHSV, min_HSV, max_HSV)\n",
    "\n",
    "skinHSV = cv2.bitwise_and(image, image, mask = skinRegionHSV)\n",
    "\n",
    "\n",
    "\n",
    "# ret1,th1 = cv.threshold(img,127,255,cv.THRESH_BINARY)\n",
    "# # Otsu's thresholding\n",
    "# ret2,th2 = cv.threshold(img,0,255,cv.THRESH_BINARY+cv.THRESH_OTSU)\n",
    "# # Otsu's thresholding after Gaussian filtering\n",
    "blur = cv.GaussianBlur(img,(5,5),0)\n",
    "ret3,th3 = cv.threshold(blur,0,255,cv.THRESH_BINARY+cv.THRESH_OTSU)\n",
    "\n",
    "# # plot all the images and their histograms\n",
    "\n",
    "cv2.imwrite(\"masked.jpg\", th3)\n",
    "cv2.imwrite(\"masked2.jpg\", np.hstack([image,skinYCrCb]))\n",
    "cv2.imwrite(\"masked3.jpg\", np.hstack([image,skinHSV]))\n",
    "\n",
    "print (th3.shape)\n",
    "print (image.shape)\n",
    "print (skinYCrCb.shape)\n",
    "print (skinHSV.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cv2.imwrite(\"masked2.jpg\", np.hstack([image,skinYCrCb]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def convolve(B, r):\n",
    "#     D = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(r,r))\n",
    "#     cv2.filter2D(B, -1, D, B)\n",
    "#     return B\n",
    "\n",
    "# #Loading the image and converting to HSV\n",
    "# image = cv2.imread(\"D:/pattern/research/dataset_sample/men/3/3_men (2).JPG\")\n",
    "# image_hsv = cv2.cvtColor(image,cv2.COLOR_BGR2HSV)\n",
    "# model_hsv = image_hsv[225:275,625:675] # Select ROI\n",
    "\n",
    "# #Get the model histogram M\n",
    "# M = cv2.calcHist([model_hsv], channels=[0, 1], mask=None, \n",
    "#                   histSize=[80, 256], ranges=[0, 180, 0, 256] )\n",
    "\n",
    "# #Backprojection of our original image using the model histogram M\n",
    "# B = cv2.calcBackProject([image_hsv], channels=[0,1], hist=M, \n",
    "#                          ranges=[0,180,0,256], scale=1)\n",
    "\n",
    "# B = convolve(B, r=5)\n",
    "\n",
    "# #Threshold to clean the image and merging to three-channels\n",
    "# _, thresh = cv2.threshold(B, 0, 255, cv2.THRESH_BINARY)\n",
    "# cv2.imwrite(\"masked.jpg\",cv2.cvtColor(model_hsv,cv2.COLOR_HSV2RGB))\n",
    "# cv2.imwrite(\"masked2.jpg\",cv2.bitwise_and(image,image, mask = thresh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required modules\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "min_HSV = np.array([0, 58, 30], dtype = \"uint8\")\n",
    "max_HSV = np.array([33, 255, 255], dtype = \"uint8\")\n",
    "# Get pointer to video frames from primary device\n",
    "image = cv2.imread(\"D:/pattern/research/dataset_sample/men/3/3_men (2).JPG\")\n",
    "imageHSV = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "skinRegionHSV = cv2.inRange(imageHSV, min_HSV, max_HSV)\n",
    "\n",
    "skinHSV = cv2.bitwise_and(image, image, mask = skinRegionHSV)\n",
    "\n",
    "cv2.imwrite(\"masked.jpg\", np.hstack([image, skinHSV]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
