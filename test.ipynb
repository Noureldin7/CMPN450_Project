{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import math\n",
    "import glob\n",
    "from scipy import integrate\n",
    "from sklearn.cluster import MeanShift, estimate_bandwidth, KMeans\n",
    "# from skimage.filters import sobel\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = cv2.imread(\"clean_reduced_dataset/men/1/1_men (92).JPG\")\n",
    "# img = cv2.cvtColor(img,cv2.COLOR_BGR2YCrCb)\n",
    "# # img = cv2.boxFilter(img,-1,(10,10))\n",
    "# mask = cv2.inRange(img,(0,100,67),(255,255,255))\n",
    "# img[mask==0]=[0,0,0]\n",
    "# img = cv2.cvtColor(img,cv2.COLOR_YCrCb2BGR)\n",
    "img = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "dx = cv2.Sobel(img,cv2.CV_64F,1,0,ksize=9)\n",
    "dy = cv2.Sobel(img,cv2.CV_64F,0,1,ksize=9)\n",
    "img_masked = np.linalg.norm(np.array([dx,dy]),axis=0)\n",
    "img_masked *= 255.0/np.max(img_masked)\n",
    "# img_masked = np.asarray(cv2.threshold(img_masked,thresh=50,maxval=255,type=cv2.THRESH_BINARY)[1],dtype=np.uint8)\n",
    "\n",
    "# ctrs, _ = cv2.findContours(img_masked,cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_NONE)\n",
    "# max_ctr = max(ctrs,key=cv2.contourArea)\n",
    "# contour = np.zeros(img_masked.shape)\n",
    "# cv2.drawContours(contour,max_ctr,-1,(255,0,0),1)\n",
    "# histogramV = np.mean(np.sum(img_masked,axis=0))\n",
    "# print(histogramV)\n",
    "cv2.imwrite(\"masked.JPG\",img_masked)\n",
    "# cv2.imwrite(\"masked.JPG\",img_masked)\n",
    "# print(img_masked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[376], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m max_ctr \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(ctrs,key\u001b[39m=\u001b[39mcv2\u001b[39m.\u001b[39mcontourArea)\n\u001b[0;32m     13\u001b[0m contour \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros(img_masked\u001b[39m.\u001b[39mshape)\n\u001b[1;32m---> 14\u001b[0m cv2\u001b[39m.\u001b[39;49mdrawContours(contour,max_ctr,\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m,(\u001b[39m255\u001b[39;49m,\u001b[39m0\u001b[39;49m,\u001b[39m0\u001b[39;49m),\u001b[39m1\u001b[39;49m)\n\u001b[0;32m     15\u001b[0m cv2\u001b[39m.\u001b[39mimwrite(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmasks/\u001b[39m\u001b[39m{\u001b[39;00mfilename[\u001b[39m28\u001b[39m:]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m,contour)\n\u001b[0;32m     16\u001b[0m x\u001b[39m+\u001b[39m\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "x=0\n",
    "for filename in sorted(glob.glob('clean_reduced_dataset/men/*/*.JPG')):\n",
    "    img = cv2.imread(filename)\n",
    "    img = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    dx = cv2.Sobel(img,cv2.CV_64F,1,0,ksize=9)\n",
    "    dy = cv2.Sobel(img,cv2.CV_64F,0,1,ksize=9)\n",
    "    img_masked = np.linalg.norm(np.array([dx,dy]),axis=0)\n",
    "    img_masked *= 255.0/np.max(img_masked)\n",
    "    img_masked = np.asarray(cv2.threshold(img_masked,thresh=5,maxval=255,type=cv2.THRESH_BINARY)[1],dtype=np.uint8)\n",
    "\n",
    "    ctrs, _ = cv2.findContours(img_masked,cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_NONE)\n",
    "    max_ctr = max(ctrs,key=cv2.contourArea)\n",
    "    contour = np.zeros(img_masked.shape)\n",
    "    cv2.drawContours(contour,max_ctr,-1,(255,0,0),1)\n",
    "    cv2.imwrite(f\"masks/{filename[28:]}\",contour)\n",
    "    x+=1\n",
    "# plt.imshow(img_masked)\n",
    "# np.array((0,137,77),dtype=np.uint8)\n",
    "# img_masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.cvtColor(cv2.imread(\"../reduced_dataset/men/4/4_men (141).JPG\"),cv2.COLOR_BGR2YCrCb)\n",
    "# plt.imshow(img)\n",
    "# img = cv2.resize(src=img,dsize=(int(img.shape[1]/8),int(img.shape[0]/8)))\n",
    "# img = cv2.bilateralFilter(img,d=5,sigmaColor=10,sigmaSpace=10)\n",
    "img_vector = img.reshape([-1,3])\n",
    "# img_vector = img_vector[:,1:3]\n",
    "img_vector[:,0] = 100\n",
    "# img_vector[:,2] = img_vector[:,2] * 12\n",
    "# img_vector[:,1] = img_vector[:,1] * 6\n",
    "# img_vector_YExcluded = img_vector[:,1:]\n",
    "bw = estimate_bandwidth(X=img_vector,quantile=0.1,n_samples=500)\n",
    "# bw\n",
    "print(bw)\n",
    "ms = MeanShift(bandwidth=bw,bin_seeding=True)\n",
    "ms.fit(img_vector)\n",
    "print(ms.cluster_centers_.shape)\n",
    "# img\n",
    "\n",
    "# img_vector[:,1:3] = img_vector[:,1:3] / 5\n",
    "result_image = ms.cluster_centers_.astype(dtype=np.uint8)[ms.labels_].reshape((img.shape[0],img.shape[1],3))\n",
    "# result_image = ms.cluster_centers_.astype(dtype=np.uint8)[ms.labels_].reshape((img.shape[0],img.shape[1],2))\n",
    "# result_image = cv2.cvtColor(result_image,cv2.COLOR_YCrCb2BGR)\n",
    "# result_image = cv2.inRange(result_image,(0,137,77),(255,163,140))\n",
    "# result_image[:,2] = result_image[:,2] / 12\n",
    "# result_image[:,1] = result_image[:,1] / 6\n",
    "# result_image = np.concatenate(np.ones((img.shape[0], img.shape[1],1),dtype=np.uint8)*128,result_image)\n",
    "cv2.imwrite(\"compressed.JPG\",result_image)\n",
    "# plt.imshow(result_image)\n",
    "# print(ms.cluster_centers_[ms.labels_])\n",
    "# fig = plt.figure(figsize=(12,10))\n",
    "# ax = fig.add_subplot(111,projection='3d')\n",
    "# ax.scatter(img[0,:,0],img[0,:,1],img[0,:,2])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.cvtColor(cv2.imread(\"../reduced_dataset/men/4/4_men (141).JPG\"),cv2.COLOR_BGR2YCrCb)\n",
    "bg_mask = np.bitwise_not(cv2.inRange(img,(0,137,77),(255,163,120))).reshape((-1))\n",
    "fg_mask = cv2.inRange(img,(0,137,77),(255,163,120)).reshape((-1))\n",
    "# print(bg_mask.shape)\n",
    "# print(img.reshape((-1,3)).shape)\n",
    "# print(img.reshape((-1,3))[bg_mask == 255].shape)\n",
    "bg_mean = np.mean(img.reshape((-1,3))[bg_mask == 255], axis = 0)\n",
    "fg_mean = np.mean(img.reshape((-1,3))[fg_mask != 0], axis = 0)\n",
    "print(bg_mean)\n",
    "print(fg_mean)\n",
    "\n",
    "# plt.imshow(img)\n",
    "# img = cv2.resize(src=img,dsize=(int(img.shape[1]/8),int(img.shape[0]/8)))\n",
    "# img = cv2.bilateralFilter(img,d=5,sigmaColor=10,sigmaSpace=10)\n",
    "img_vector = img.reshape([-1,3])\n",
    "# img_vector[:,0] = 100\n",
    "divisor = 20\n",
    "img_vector[:,1] = (np.tanh((img_vector[:,1] - 137)/divisor) - np.tanh((img_vector[:,1] - 163)/divisor)) * img_vector[:,1]/2\n",
    "img_vector[:,2] = (np.tanh((img_vector[:,2] - 77)/divisor) - np.tanh((img_vector[:,2] - 120)/divisor)) * img_vector[:,2]/2\n",
    "# (0,137,77),(255,163,120)\n",
    "# cv2.imwrite(\"compressed.JPG\",img_vector.reshape((img.shape[0],img.shape[1],3)))\n",
    "\n",
    "\n",
    "# km = KMeans(n_clusters=3)\n",
    "km = KMeans(n_clusters=2, init=np.array([[bg_mean[0],0,0], fg_mean]), max_iter=3)\n",
    "# km = KMeans(n_clusters=3, init=np.array([bg_mean,[30, 149, 96],[200, 149, 96]]), max_iter=3)\n",
    "km.fit(img_vector)\n",
    "\n",
    "result_image = km.cluster_centers_.astype(dtype=np.uint8)[km.labels_].reshape((img.shape[0],img.shape[1],3))\n",
    "# result_image = cv2.cvtColor(result_image,cv2.COLOR_YCrCb2BGR)\n",
    "cv2.imwrite(\"compressed.JPG\",result_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread(\"clean_reduced_dataset/men/0/0_men (57).JPG\")\n",
    "# print(img.dtype)\n",
    "img = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "img = np.log(img+0.001)\n",
    "# img = np.exp(img)\n",
    "# print(img)\n",
    "# img = np.linalg.norm(img,axis=2)\n",
    "grad = np.asarray(np.gradient(img,axis=(0,1)))\n",
    "# grad = np.linalg.norm(img,axis=3)\n",
    "invariant_grad = cv2.threshold(grad,thresh=0.3,maxval=0,type=cv2.THRESH_TOZERO_INV)[1]\n",
    "mask1 = [cv2.inRange(grad[0],0,0),cv2.inRange(grad[1],0,0)]\n",
    "mask2 = [cv2.inRange(invariant_grad[0],0,0),cv2.inRange(invariant_grad[1],0,0)]\n",
    "S = grad\n",
    "S[0] = cv2.bitwise_and(grad[0],grad[0],cv2.bitwise_or(mask1[0],mask2[0]))\n",
    "S[1] = cv2.bitwise_and(grad[1],grad[1],cv2.bitwise_or(mask1[1],mask2[1]))\n",
    "# print(grad)\n",
    "# print(S)\n",
    "# divS = np.asarray(np.gradient(S[0],axis=0) + np.gradient(S[1],axis=1))\n",
    "# print(mask1)\n",
    "final_result = np.asarray(np.exp(integrate.cumtrapz(invariant_grad[0],axis=0)+4.62))\n",
    "# print(final_result)\n",
    "# img_masked = img\n",
    "# # img = img.astype(\"int64\")\n",
    "# img = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "# dx = cv2.Sobel(img,cv2.CV_64F,1,0,ksize=5)\n",
    "# dy = cv2.Sobel(img,cv2.CV_64F,0,1,ksize=5)\n",
    "# img_masked = np.hypot(dx,dy)\n",
    "# img_masked *= 255.0/np.max(img_masked)\n",
    "# print(img_masked)\n",
    "# img = cv2.cvtColor(img,cv2.COLOR_BGR2YCrCb)\n",
    "# img_gray = cv2.inRange(cv2.cvtColor(img,cv2.COLOR_BGR2GRAY),0,128)\n",
    "# print(img_gray)\n",
    "# brightness_coeff = (np.mean(img_gray)/255 - 0.015)*10\n",
    "# print(brightness_coeff) #0.016------0.6\n",
    "# # img_masked = cv2.inRange(img,(0,int(142+brightness_coeff),77),(255,163,140))\n",
    "# img_masked = cv2.inRange(img,(100,137,77),(255,163,140))\n",
    "# img = cv2.cvtColor(img,cv2.COLOR_BGR2HSV)\n",
    "# img_masked_shadow = np.bitwise_not(cv2.inRange(img,(0,0,80),(255,55,178)))\n",
    "# img_masked_hand = cv2.inRange(img,(5,35,30),(25,255,255))\n",
    "# img_masked = np.bitwise_and(img_masked_shadow,img_masked_hand)\n",
    "cv2.imwrite(\"masked.JPG\",final_result)\n",
    "del img\n",
    "# del img_masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "img = cv.imread('D:/pattern/research/dataset_sample/men/3/3_men (2).JPG', cv.IMREAD_GRAYSCALE)\n",
    "assert img is not None, \"file could not be read, check with os.path.exists()\"\n",
    "# global thresholding\n",
    "ret1,th1 = cv.threshold(img,127,255,cv.THRESH_BINARY)\n",
    "# Otsu's thresholding\n",
    "ret2,th2 = cv.threshold(img,0,255,cv.THRESH_BINARY+cv.THRESH_OTSU)\n",
    "# Otsu's thresholding after Gaussian filtering\n",
    "blur = cv.GaussianBlur(img,(5,5),0)\n",
    "ret3,th3 = cv.threshold(blur,0,255,cv.THRESH_BINARY+cv.THRESH_OTSU)\n",
    "# plot all the images and their histograms\n",
    "images = [img, 0, th1,\n",
    "          img, 0, th2,\n",
    "          blur, 0, th3]\n",
    "titles = ['Original Noisy Image','Histogram','Global Thresholding (v=127)',\n",
    "          'Original Noisy Image','Histogram',\"Otsu's Thresholding\",\n",
    "          'Gaussian filtered Image','Histogram',\"Otsu's Thresholding\"]\n",
    "for i in range(3):\n",
    "    plt.subplot(3,3,i*3+1),plt.imshow(images[i*3],'gray')\n",
    "    plt.title(titles[i*3]), plt.xticks([]), plt.yticks([])\n",
    "    plt.subplot(3,3,i*3+2),plt.hist(images[i*3].ravel(),256)\n",
    "    plt.title(titles[i*3+1]), plt.xticks([]), plt.yticks([])\n",
    "    plt.subplot(3,3,i*3+3),plt.imshow(images[i*3+2],'gray')\n",
    "    plt.title(titles[i*3+2]), plt.xticks([]), plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import time\n",
    "\n",
    "# imgWidth = 1280\n",
    "# imgHeight = 720\n",
    "# cap = cv2.VideoCapture(0)\n",
    "# cap.set(3, imgWidth)\n",
    "# cap.set(4, imgHeight)\n",
    "mpHands=mp.solutions.hands\n",
    "hands=mpHands.Hands()\n",
    "mpDraw = mp.solutions.drawing_utils\n",
    "\n",
    "# pTime = 0\n",
    "# cTime = 0\n",
    "filename = 'data_set/men/3/3_men (5).JPG'\n",
    "img = cv2.imread(filename)\n",
    "img= cv2.flip(img,1)\n",
    "img_b = np.empty(img.shape)\n",
    "img_b.fill(0)\n",
    "imgRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "results = hands.process(imgRGB)\n",
    "#print(results.multi_hand_landmarks)\n",
    "if results.multi_hand_landmarks:\n",
    "    for handLms in results.multi_hand_landmarks: \n",
    "    #handLMs are 21 points. so we need conection too-->mpHands.HAND_CONNECTIONS\n",
    "        for id, lm in enumerate(handLms.landmark):\n",
    "            #print(id, lm)\n",
    "            #lm = x,y cordinate of each landmark in float numbers. lm.x, lm.y methods\n",
    "            #So, need to covert in integer\n",
    "            h, w, c =img.shape\n",
    "            cx, cy = int(lm.x * w), int(lm.y * h)\n",
    "            #print(id, cx, cy)\n",
    "            # if id == 4: #(To draw 4th point)\n",
    "            #cv2.circle(img, (cx, cy), 15, (255, 0, 255), cv2.FILLED)\n",
    "        mpDraw.draw_landmarks(img_b, handLms, mpHands.HAND_CONNECTIONS) #drawing points and lines(=handconections)\n",
    "\n",
    "\n",
    "cv2.imwrite('masked.JPG', img_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Parameters\n",
    "# blur = 21\n",
    "# canny_low = 15\n",
    "# canny_high = 150\n",
    "# min_area = 0.0005\n",
    "# max_area = 0.95\n",
    "# dilate_iter = 10\n",
    "# erode_iter = 10\n",
    "# mask_color = (0.0,0.0,0.0)\n",
    "# filename = 'D:/pattern/research/dataset_sample/men/3/3_men (1).JPG'\n",
    "# # Convert image to grayscale        \n",
    "# img = cv2.imread(filename) \n",
    "# image_gray = cv2.cvtColor(img,cv2.COLOR_BGR2YCrCb)\n",
    "# # Apply Canny Edge Dection\n",
    "# edges = cv2.Canny(image_gray, canny_low, canny_high)\n",
    "\n",
    "# edges = cv2.dilate(edges, None)\n",
    "# edges = cv2.erode(edges, None)\n",
    "\n",
    "# contour_info = []\n",
    "\n",
    "# c \n",
    "\n",
    "# for c in cv2.findContours(edges, cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)[0]:\n",
    "\n",
    "#     contour_info.append((\n",
    "#         c,\n",
    "#         cv2.contourArea(c),\n",
    "#     ))\n",
    "    \n",
    "\n",
    "# # get the contours and their areas\n",
    "\n",
    "\n",
    "# # Set up mask with a matrix of 0's\n",
    "# mask = np.zeros(edges.shape, dtype = np.uint8)\n",
    "\n",
    "\n",
    "# # Go through and find relevant contours and apply to mask\n",
    "# for contour in contour_info:\n",
    "# # Instead of worrying about all the smaller contours, if the area is smaller than the min, the loop will break\n",
    "#     if contour[1] > min_area and contour[1] < max_area:\n",
    "#         # Add contour to mask\n",
    "#         mask = cv2.fillConvexPoly(mask, contour[0], (255))\n",
    "\n",
    "# # use dilate, erode, and blur to smooth out the mask\n",
    "# # use dilate, erode, and blur to smooth out the mask\n",
    "# mask = cv2.dilate(mask, None)\n",
    "# mask = cv2.erode(mask, None)\n",
    "# mask = cv2.GaussianBlur(mask, (blur, blur), 0)\n",
    "\n",
    "# # Ensures data types match up\n",
    "# mask_stack = mask.astype('float32') / 255.0           \n",
    "# img = img.astype('float32') / 255.0\n",
    "\n",
    "# # Creates a 3 channel image\n",
    "\n",
    "# # Multiplies mask_stack with image to get masked image\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(mask_stack.shape)\n",
    "# print(img.shape)\n",
    "\n",
    " \n",
    "# # Blend the image and the mask\n",
    "# masked = (mask_stack * img) + ((1-mask_stack) * mask_color)\n",
    "# masked = (masked * 255).astype('uint8')\n",
    "# cv2.imwrite(\"masked.jpg\", masked)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required modules\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "min_YCrCb = np.array([0,133,77],np.uint8)\n",
    "max_YCrCb = np.array([235,173,140],np.uint8)\n",
    "\n",
    "min_HSV = np.array([0, 58, 30], dtype = \"uint8\")\n",
    "max_HSV = np.array([33, 255, 255], dtype = \"uint8\")\n",
    "\n",
    "# Get pointer to video frames from primary device\n",
    "image = cv2.imread(\"D:/pattern/research/dataset_sample/men/3/3_men (2).JPG\")\n",
    "\n",
    "imageYCrCb = cv2.cvtColor(image,cv2.COLOR_BGR2YCR_CB)\n",
    "skinRegionYCrCb = cv2.inRange(imageYCrCb,min_YCrCb,max_YCrCb)\n",
    "\n",
    "skinYCrCb = cv2.bitwise_and(image, image, mask = skinRegionYCrCb)\n",
    "\n",
    "\n",
    "imageHSV = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "skinRegionHSV = cv2.inRange(imageHSV, min_HSV, max_HSV)\n",
    "\n",
    "skinHSV = cv2.bitwise_and(image, image, mask = skinRegionHSV)\n",
    "\n",
    "\n",
    "\n",
    "# ret1,th1 = cv.threshold(img,127,255,cv.THRESH_BINARY)\n",
    "# # Otsu's thresholding\n",
    "# ret2,th2 = cv.threshold(img,0,255,cv.THRESH_BINARY+cv.THRESH_OTSU)\n",
    "# # Otsu's thresholding after Gaussian filtering\n",
    "blur = cv.GaussianBlur(img,(5,5),0)\n",
    "ret3,th3 = cv.threshold(blur,0,255,cv.THRESH_BINARY+cv.THRESH_OTSU)\n",
    "\n",
    "# # plot all the images and their histograms\n",
    "\n",
    "cv2.imwrite(\"masked.jpg\", th3)\n",
    "cv2.imwrite(\"masked2.jpg\", np.hstack([image,skinYCrCb]))\n",
    "cv2.imwrite(\"masked3.jpg\", np.hstack([image,skinHSV]))\n",
    "\n",
    "print (th3.shape)\n",
    "print (image.shape)\n",
    "print (skinYCrCb.shape)\n",
    "print (skinHSV.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "cv2.imwrite(\"masked2.jpg\", np.hstack([image,skinYCrCb]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def convolve(B, r):\n",
    "#     D = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(r,r))\n",
    "#     cv2.filter2D(B, -1, D, B)\n",
    "#     return B\n",
    "\n",
    "# #Loading the image and converting to HSV\n",
    "# image = cv2.imread(\"D:/pattern/research/dataset_sample/men/3/3_men (2).JPG\")\n",
    "# image_hsv = cv2.cvtColor(image,cv2.COLOR_BGR2HSV)\n",
    "# model_hsv = image_hsv[225:275,625:675] # Select ROI\n",
    "\n",
    "# #Get the model histogram M\n",
    "# M = cv2.calcHist([model_hsv], channels=[0, 1], mask=None, \n",
    "#                   histSize=[80, 256], ranges=[0, 180, 0, 256] )\n",
    "\n",
    "# #Backprojection of our original image using the model histogram M\n",
    "# B = cv2.calcBackProject([image_hsv], channels=[0,1], hist=M, \n",
    "#                          ranges=[0,180,0,256], scale=1)\n",
    "\n",
    "# B = convolve(B, r=5)\n",
    "\n",
    "# #Threshold to clean the image and merging to three-channels\n",
    "# _, thresh = cv2.threshold(B, 0, 255, cv2.THRESH_BINARY)\n",
    "# cv2.imwrite(\"masked.jpg\",cv2.cvtColor(model_hsv,cv2.COLOR_HSV2RGB))\n",
    "# cv2.imwrite(\"masked2.jpg\",cv2.bitwise_and(image,image, mask = thresh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required modules\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "min_HSV = np.array([0, 58, 30], dtype = \"uint8\")\n",
    "max_HSV = np.array([33, 255, 255], dtype = \"uint8\")\n",
    "# Get pointer to video frames from primary device\n",
    "image = cv2.imread(\"D:/pattern/research/dataset_sample/men/3/3_men (2).JPG\")\n",
    "imageHSV = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "skinRegionHSV = cv2.inRange(imageHSV, min_HSV, max_HSV)\n",
    "\n",
    "skinHSV = cv2.bitwise_and(image, image, mask = skinRegionHSV)\n",
    "\n",
    "cv2.imwrite(\"masked.jpg\", np.hstack([image, skinHSV]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
